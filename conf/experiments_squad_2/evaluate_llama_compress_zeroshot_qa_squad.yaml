# @package _global_
defaults:
  - override /custom_datasets: squad_v2_for_llama_custom
  - override /models: quantized_causal_llama_compress_model
  - override /tokenizers: causal_auto_tokenizer
  - override /trainers: trainer
  - override /predictors: qa_lm_predictor

custom_datasets:
  test:
    data_config:
      context_max_length: 3808

models:
  pretrained_model_name_or_path: "meta-llama/Llama-2-7b-chat-hf"
  mode: "attention_score"
  compression_factor: 1.0
  split_size: 512
  target_token: 256
  distance_metric: None
  config:
    pretrained_model_name_or_path: "meta-llama/Llama-2-7b-chat-hf"

tokenizers:
  pretrained_model_name_or_path: "meta-llama/Llama-2-7b-chat-hf"

predictors:
  predictor_config:
    task_type: "CLM"
    batch_size: ${trainers.evaluation_config.per_device_eval_batch_size}


trainers:
  mode: "eval"
  evaluation_config:
    with_tracking: True
    output_dir: ${oc.env:LOGS_ROOT}/eval_zeroshot_squadv2_qa_lm_llama_7b-chat_target_token${models.target_token}_split_size${models.split_size}_new_fix_kfixed
    per_device_eval_batch_size: 1
    resume_from_checkpoint: False
    zero_shot: True

  logging_config:
    project_name: "qa_squad_v2"
    name: ${trainers.evaluation_config.output_dir}

# @package _global_
defaults:
  - override /custom_datasets: squad_v2
  - override /models: qa_model
  - override /tokenizers: auto_tokenizer
  - override /trainers: trainer
  - override /predictors: qa_predictor

models:
    pretrained_model_name_or_path: "google/flan-t5-base"
    config:
        pretrained_model_name_or_path: "google/flan-t5-base"

tokenizers:
    pretrained_model_name_or_path: "google/flan-t5-base"

trainers:
  training_config:
    output_dir: ${oc.env:LOGS_ROOT}/fine_tuning_squad_v2_qa_vanilla_t5_base
    with_tracking: True
    resume_from_checkpoint: False
    checkpointing_steps: "10000"
    per_device_train_batch_size: 4
    per_device_val_batch_size: 4
    total_steps: 100000
    gradient_accumulation_steps: 1
  optimization_config:
    learning_rate": 1e-5
    weight_decay: 0.03 # Weight decay to use.
    scheduler_name: "cosine" # The scheduler type to use. "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"
    num_warmup_steps: 5000 # Number of steps for the warmup in the lr scheduler.

  optimizer:
    _target_: "torch.optim.AdamW"

  logging_config:
    project_name: "qa_squad_v2"
    name: ${trainers.training_config.output_dir}
